{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import some usefull libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement function from first exercise and modify it so we don't have capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text: str) -> list:\n",
    "    list_words = []\n",
    "    aux = ''\n",
    "    for i in range(len(text)):\n",
    "        aux += str(text[i])\n",
    "        if i != len(text)-1:\n",
    "            if text[i] in ' ' and aux[:-1] != '':\n",
    "                aux = aux[:-1]\n",
    "                list_words.append(aux.lower())\n",
    "                aux = ''\n",
    "            elif text[i] in '[.,!?]':\n",
    "                if text[i+1] != '.' and text[i+1].islower() == False:\n",
    "                    aux = aux[:-1]\n",
    "                    list_words.append(aux.lower())\n",
    "                    aux = ''\n",
    "        else:\n",
    "            if text[i] in '[.,!?]':\n",
    "                aux = aux[:-1]\n",
    "            list_words.append(aux)\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define _Bag of words_ class, where we fit the transform and get features names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 2. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 1. 1. 1. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BagOfWords:    \n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        N_sentences = len(corpus)\n",
    "        words = []; unique_words = []; unique_words_count = []\n",
    "        \n",
    "        for i in range(N_sentences):\n",
    "            words += tokenize_words(corpus[i])\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "                unique_words_count.append(words.count(word))\n",
    "        \n",
    "        unique_words_count = np.array(unique_words_count); unique_words = np.array(unique_words)\n",
    "        unique_words = unique_words[np.flipud(unique_words_count.argsort())] # Sort it by frequency\n",
    "        \n",
    "        self.unique_words = unique_words\n",
    "        \n",
    "        N_unique_words = len(unique_words)\n",
    "        matrix = np.zeros([N_sentences, N_unique_words])\n",
    "        \n",
    "        for sentence in range(N_sentences):\n",
    "            for word_i in range(N_unique_words):\n",
    "                matrix[sentence, word_i] = tokenize_words(corpus[sentence]).count(unique_words[word_i])\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def get_feature_names(self) -> list: \n",
    "        return self.unique_words\n",
    "\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]    \n",
    "    \n",
    "vectorizer = BagOfWords()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "\n",
    "vectorizer.get_feature_names()\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we have the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['words', 'below', ' see', 'of', 'is', 'the', 'this', 'multiple',\n",
       "       'third', 'documents', ' really', 'throughout', 'occurences',\n",
       "       'document', 'on', 'based', 'counting', 'as', 'you', 'can', 'see',\n",
       "       'most', 'occur', 'only', 'once', 'gives', 'us', 'a', 'pretty',\n",
       "       'sparse', 'matrix', 'bag'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
